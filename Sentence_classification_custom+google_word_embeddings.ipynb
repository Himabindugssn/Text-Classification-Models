{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Himabindugssn/Text-Classification-Models/blob/main/Sentence_classification_custom%2Bgoogle_word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSC3TN1Ed3-Z"
      },
      "outputs": [],
      "source": [
        "#http://ieeexplore.ieee.org.dtulibrary.remotexs.in/document/8314136 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GshAy9D8WFSg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import *\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Model,Sequential\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.math import l2_normalize\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omCiC2qZbwh0",
        "outputId": "1b1e963d-dd1c-43e7-f3b7-229b175b0670"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFle7wqA2TDR"
      },
      "source": [
        "# Loading Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZobqJQInHPm8"
      },
      "source": [
        "After using GLOVE, you will find that certain words in your training data are not present in its vocab. These are typically replaced with same-shape zero vector, which essentially means you are 'sacrificing' the word as your input feature, which can potentially be important for correct prediction. Another way to deal with this is to train your own word embeddings, using your training data, so that the semantic relationship of your own training corpus can be better represented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfyj0EyYIYtO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('/content/drive/MyDrive/IMDB Dataset.csv', error_bad_lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2dLz-KnLfX0",
        "outputId": "7ee1653a-ef8e-4005-8532-5046919bf52b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50000, 2)"
            ]
          },
          "execution_count": 4,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FD00Pz0rWQ-0",
        "outputId": "653c9983-5b98-46b0-950d-2bb67cf047c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(xtrain,ytrain),(xtest,ytest)=imdb.load_data(num_words=5000) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGnPUYVy9SIv",
        "outputId": "7e8ebfd6-7515-4bfc-d927-8abae1602e71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> 25000\n"
          ]
        }
      ],
      "source": [
        "print(type(xtrain),len(xtest))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrZcfeyeWRzN",
        "outputId": "caa8eb90-de9f-4859-aaa3-eb275baf3eff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "word_idx=imdb.get_word_index() #getting vocab from imdb data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJ0eIyNvAVAb"
      },
      "outputs": [],
      "source": [
        "#len(word_idx.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKxRtcYeOTQf",
        "outputId": "ba8a7b9f-0079-40ef-fc72-43c52d0ca5c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiOHzhivMBcI"
      },
      "outputs": [],
      "source": [
        "#Load the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "import re,string,unicodedata\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
        "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from textblob import TextBlob\n",
        "from textblob import Word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DgFiccjMRqB",
        "outputId": "4c8b0f0f-d96c-4607-c8a0-8bf33b867715"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#Tokenization of text\n",
        "tokenizer=ToktokTokenizer()\n",
        "#Setting English stopwords\n",
        "stopword_list=nltk.corpus.stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hDhbLYfMSIj"
      },
      "outputs": [],
      "source": [
        "#Removing the html strips\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "#Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "#Removing the noisy text\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXJpN9-TMbjP",
        "outputId": "bd65daa2-c810-40fc-825c-2a8be3e5b61d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['review', 'sentiment'], dtype='object')"
            ]
          },
          "execution_count": 13,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aduucmGvMWaa"
      },
      "outputs": [],
      "source": [
        "df['review']=df['review'].apply(denoise_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFLsjko5MgK_"
      },
      "outputs": [],
      "source": [
        "#Define function for removing special characters\n",
        "def remove_special_characters(text, remove_digits=True):\n",
        "    pattern=r'[^a-zA-z0-9\\s]'\n",
        "    text=re.sub(pattern,'',text)\n",
        "    return text\n",
        "    \n",
        "#Apply function on review column\n",
        "df['review']=df['review'].apply(remove_special_characters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_P80Lj_MlE9"
      },
      "outputs": [],
      "source": [
        "#Stemming the text\n",
        "def simple_stemmer(text):\n",
        "    ps=nltk.porter.PorterStemmer()\n",
        "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "#Apply function on review column\n",
        "#df['review']=df['review'].apply(simple_stemmer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvF_pBvjMppD"
      },
      "outputs": [],
      "source": [
        "#set stopwords to english\n",
        "stopword_list =set(stopwords.words('english'))\n",
        "\n",
        "#removing the stopwords\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "\n",
        "#Apply function on review column\n",
        "df['review']= df['review'].apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WXGjOs7O_37",
        "outputId": "97e399f2-a353-4cfb-e933-a95f0511362b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50000/50000 [00:00<00:00, 52389.90it/s]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from tqdm import tqdm\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "sentences = df['review']\n",
        "train_sentences = list(sentences.progress_apply(str.split).values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24Y1kXsePa0-"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Parameters reference : https://www.quora.com/How-do-I-determine-Word2Vec-parameters\n",
        "# Feel free to customise your own embedding\n",
        "\n",
        "#start_time = time.time()\n",
        "\n",
        "\n",
        "#model = Word2Vec(sentences=train_sentences, \n",
        " #                sg=1, \n",
        " #                size=300,  \n",
        " #                workers=4)\n",
        "\n",
        "#print(f'Time taken : {(time.time() - start_time) / 60:.2f} mins')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItGfZ9OFPoRd"
      },
      "outputs": [],
      "source": [
        "#model.wv.vector_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgXfYeb4SnM1"
      },
      "outputs": [],
      "source": [
        "#model.wv.vocab.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyYFRjDuQfbd"
      },
      "outputs": [],
      "source": [
        "#model.wv.save_word2vec_format('custom_glove_100d.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqlyTCUZbe0R"
      },
      "outputs": [],
      "source": [
        "maxlen=500\n",
        "vocab_size=5000\n",
        "emb_dimension=300\n",
        "xtrain=pad_sequences(xtrain,maxlen=maxlen,padding='post')\n",
        "xtest=pad_sequences(xtest,maxlen=maxlen,padding='post')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgBdMY4G2f38"
      },
      "source": [
        "# Loading Google's pretrained word2vec model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3YGeVpbtjcV",
        "outputId": "12f288cd-1fb6-404b-c92e-f6d3e8f9ab90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2020-11-26 07:28:21--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.104.13\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.104.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘download/GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  49.5MB/s    in 22s     \n",
            "\n",
            "2020-11-26 07:28:44 (70.0 MB/s) - ‘download/GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -P download -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "from gensim.models import KeyedVectors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I27nbGV6dD8Z"
      },
      "outputs": [],
      "source": [
        "model1 = KeyedVectors.load_word2vec_format('download/GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0j5r0O1mHwGX"
      },
      "outputs": [],
      "source": [
        "for i in model1.vocab.keys():\n",
        "  train_sentences.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCIu6heTH1Xm",
        "outputId": "15ac5883-75b1-41f4-c1b7-f54c16efff7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3050000"
            ]
          },
          "execution_count": 29,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huPIOLf189QU"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "model = gensim.models.Word2Vec(train_sentences,size=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVtABTErDHM4"
      },
      "outputs": [],
      "source": [
        "#model1=KeyedVectors.load_word2vec_format('download/GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCO7kAai9Nat",
        "outputId": "908c4425-11e1-46dc-ab1f-1f5af0e89046"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "51070"
            ]
          },
          "execution_count": 31,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(model.wv.vocab.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w91eexmZDJgx",
        "outputId": "3bd14c74-1bfe-423e-cd20-86a0169dfc93"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "execution_count": 32,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.wv.vector_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAFAzrpwFLpO"
      },
      "outputs": [],
      "source": [
        "#model.intersect_word2vec_format(model1.vocab.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38h1FcJlArTA"
      },
      "outputs": [],
      "source": [
        "#model.intersect_word2vec_format('download/GoogleNews-vectors-negative300.bin.gz',lockf=1.0,binary=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOxaTESGAs8p"
      },
      "outputs": [],
      "source": [
        "#model2=model.train(train_sentences,total_words=3050685,epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOgV2MYZ2isZ"
      },
      "source": [
        "# Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RKP5x7Ctm-S"
      },
      "outputs": [],
      "source": [
        "embedding_matrix = np.zeros((vocab_size,emb_dimension))\n",
        "for word, i in word_idx.items():\n",
        "  if word in model.wv.vocab.keys() and i<vocab_size:  #if word in word2vec.vocab.keys() and i<vocab_size:\n",
        "        embedding_matrix[i] = model.wv.word_vec(word) #embedding_matrix[i] = word2vec.word_vec(word) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEb379DK-r5p",
        "outputId": "de037f0b-43d1-4a7a-e404-06c2083052e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5000, 300)"
            ]
          },
          "execution_count": 34,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gv0x3UEyquaf"
      },
      "outputs": [],
      "source": [
        "np.save('drive/MyDrive/emb_custom.npy',embedding_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyNGzaYdrCee"
      },
      "outputs": [],
      "source": [
        "embedding_matrix=np.load('drive/MyDrive/emb_custom.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beSSQsmyWSN8"
      },
      "outputs": [],
      "source": [
        "xtrain=np.asarray(xtrain).astype(np.float32)\n",
        "xtest=np.asarray(xtest).astype('float32')\n",
        "ytrain=np.asarray(ytrain).astype('float32')\n",
        "ytest=np.asarray(ytest).astype('float32')\n",
        "\n",
        "train=tf.data.Dataset.from_tensor_slices((xtrain,ytrain))\n",
        "test=tf.data.Dataset.from_tensor_slices((xtest,ytest))   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hELuyGki6dAz"
      },
      "outputs": [],
      "source": [
        "train=train.batch(128)\n",
        "test=test.batch(128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw5PvLC7TtwJ"
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PYQN7xhLA6n"
      },
      "outputs": [],
      "source": [
        "class MyModel(Model):\n",
        "\n",
        "  def __init__(self,vocab_size,emb_dimension,embedding_matrix,filters,kernel_size):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.emb=Embedding(vocab_size,emb_dimension,weights=[embedding_matrix],trainable=True)\n",
        "    self.conv=Conv1D(filters=filters, kernel_size=kernel_size,activation='relu',kernel_regularizer=tf.keras.regularizers.l2(l=0.01))\n",
        "    self.bn=BatchNormalization()\n",
        "    self.drop=Dropout(0.5)\n",
        "    self.lstm=GRU(128)\n",
        "    self.dense=Dense(1,activation='sigmoid')\n",
        "\n",
        "  def call(self,input):\n",
        "    x=self.emb(input)\n",
        "    x=self.conv(x)\n",
        "    x=self.bn(x)\n",
        "    x=self.drop(x)    \n",
        "    x=self.lstm(x)\n",
        "    x=self.dense(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaaG3NTag8t-"
      },
      "outputs": [],
      "source": [
        "model=MyModel(vocab_size=vocab_size,emb_dimension=emb_dimension,embedding_matrix=embedding_matrix,filters=128,kernel_size=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEWzgNfsunF0"
      },
      "outputs": [],
      "source": [
        "train_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
        "val_acc_metric = tf.keras.metrics.BinaryAccuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i642WGoCp2YW"
      },
      "outputs": [],
      "source": [
        "loss=tf.keras.losses.BinaryCrossentropy()\n",
        "#optimizer=RMSprop(learning_rate=0.01)\n",
        "epoch_losses = []\n",
        "total_epochs=8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9rf1yXOKpdN"
      },
      "outputs": [],
      "source": [
        "def model_training():\n",
        "  for epoch in range(total_epochs): \n",
        "    batch_losses=[] \n",
        "    for inputs, outputs in train:\n",
        "      with tf.GradientTape() as tape:\n",
        "        current_loss = loss(model(inputs), outputs) \n",
        "        grads=tape.gradient(current_loss, model.trainable_variables) \n",
        "      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "      batch_losses.append(current_loss) \n",
        "    \n",
        "      train_acc_metric.update_state(outputs,model(inputs))\n",
        "    \n",
        "    for inputs, outputs in test:\n",
        "      val_acc_metric.update_state(outputs,model(inputs))\n",
        "  \n",
        "    train_acc = train_acc_metric.result().numpy()\n",
        "    train_acc_metric.reset_states()\n",
        "\n",
        "    val_acc = val_acc_metric.result().numpy()\n",
        "    val_acc_metric.reset_states()\n",
        "\n",
        "    print(\"epoch \",epoch,\", Training acc : \" , train_acc,end=\"\")\n",
        "    print(\"  Validation acc: \",val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2gBftsu7DEO"
      },
      "outputs": [],
      "source": [
        "#model_training()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "IivBCRRg-Fuq",
        "outputId": "9d2d42df-f0ec-4eae-e200-1a06b20e05ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch  0 , Training acc :  0.9282366  Validation acc:  0.8545998\n",
            "epoch  1 , Training acc :  0.9314254  Validation acc:  0.8466518\n",
            "epoch  2 , Training acc :  0.9356585  Validation acc:  0.864198\n",
            "epoch  3 , Training acc :  0.93553895  Validation acc:  0.8596301\n",
            "epoch  4 , Training acc :  0.9408801  Validation acc:  0.8654098\n",
            "epoch  5 , Training acc :  0.9400431  Validation acc:  0.8560986\n",
            "epoch  6 , Training acc :  0.94076055  Validation acc:  0.86757016\n",
            "epoch  7 , Training acc :  0.9397959  Validation acc:  0.85546875\n",
            "epoch  8 , Training acc :  0.9469388  Validation acc:  0.8686623\n",
            "epoch  9 , Training acc :  0.9472178  Validation acc:  0.8691406\n",
            "epoch  10 , Training acc :  0.9461815  Validation acc:  0.8660077\n",
            "epoch  11 , Training acc :  0.9492905  Validation acc:  0.8676339\n",
            "epoch  12 , Training acc :  0.95056605  Validation acc:  0.86850286\n",
            "epoch  13 , Training acc :  0.9526786  Validation acc:  0.86589605\n",
            "epoch  14 , Training acc :  0.9540258  Validation acc:  0.86728317\n",
            "epoch  15 , Training acc :  0.95338804  Validation acc:  0.86105704\n",
            "epoch  16 , Training acc :  0.95522964  Validation acc:  0.8617188\n",
            "epoch  17 , Training acc :  0.95670444  Validation acc:  0.8687101\n",
            "epoch  18 , Training acc :  0.96053094  Validation acc:  0.86969864\n",
            "epoch  19 , Training acc :  0.9623645  Validation acc:  0.8669882\n",
            "epoch  20 , Training acc :  0.9611687  Validation acc:  0.8701451\n",
            "epoch  21 , Training acc :  0.9664302  Validation acc:  0.87336576\n",
            "epoch  22 , Training acc :  0.969619  Validation acc:  0.87244105\n",
            "epoch  23 , Training acc :  0.970177  Validation acc:  0.87311864\n",
            "epoch  24 , Training acc :  0.97121334  Validation acc:  0.8694914\n",
            "epoch  25 , Training acc :  0.9726483  Validation acc:  0.86358416\n",
            "epoch  26 , Training acc :  0.97228956  Validation acc:  0.8688935\n",
            "epoch  27 , Training acc :  0.97400355  Validation acc:  0.86735487\n",
            "epoch  28 , Training acc :  0.974442  Validation acc:  0.86674905\n",
            "epoch  29 , Training acc :  0.9754385  Validation acc:  0.8694117\n"
          ]
        }
      ],
      "source": [
        "optimizer=RMSprop(learning_rate=0.005)\n",
        "total_epochs=30\n",
        "model_training()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm5e3oj3_shj"
      },
      "outputs": [],
      "source": [
        "#optimizer=RMSprop(learning_rate=0.0001)\n",
        "total_epochs=5\n",
        "#model_training()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIjKn_xDYQbb"
      },
      "outputs": [],
      "source": [
        "#model.save_weights('drive/My Drive/Colab_files/model_100.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpFtBXN2-e5h"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgLIQBzQwC-h"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Sentence classification- custom+google word embeddings.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}